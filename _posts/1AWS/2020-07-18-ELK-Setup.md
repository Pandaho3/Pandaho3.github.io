---
title: AWS - ELK
date: 2020-07-18 11:11:11 -0400
categories: [2AWS]
tags: [AWS]
toc: true
image:
---

[toc]

---

# ELK


The data lifecycle for ELK goes a little something like this:
1. `Syslog Server` feeds `Logstash`
2. `Logstash` filters and parses logs and stores them within `Elasticsearch`
3. `Elasticsearch` indexes and makes sense out of all the data
4. `Kibana` makes millions of data points consumable by us mere mortals


For this project you will need…
1. A Linux Ubuntu Server 14.04 LTS: 1 core | 4Gb Memory | 100Gb storage (you may need more processor and memory depending on how much data you will be pumping in). http://www.ubuntu.com/download/server
2. A Palo Alto Networks firewall with a Threat Prevention Subscription
3. Something on the firewall to generate traffic


---

# Syslog

## CortexXDR

To send XDR notifications to Syslog server. define the settings for the Syslog receiver from which you want to send notifications.

1. Before define the Syslog settings, enable access to the following `XDR IP addresses` for your deployment region in your firewall configurations:

![Screen Shot 2020-11-17 at 16.53.37](https://i.imgur.com/ItiRV7H.png)

2. Navigate to Settings > Integrations > External Applications.
3. In Syslog Servers, add a <kbd>+ New Server</kbd>.
4. Define the Syslog server parameters:
   - **Name** — Unique name for the server profile.
   - **Destination** — IP address or fully qualified domain name (FQDN) of the Syslog server.
   - **Port** — The port number on which to send Syslog messages.
   - **Facility** — Choose one of the Syslog standard values.
     - The value maps to how your Syslog server uses the facility field to manage messages.
     - For details on the facility field, see RFC 5424.
   - **Protocol** — Select a method of communication with the Syslog server:
     - `TCP` — No validation is made on the connection with the Syslog server. However, if an error occurred with the domain used to make the connection, the Test connection will fail.
     - `UDP` — XDR runs a validation to ensure connection was made with the syslog server.
     - `TCP + SSL` — XDR validates the syslog server certificate and uses the certificate signature and public key to encrypt the data sent over the connection.
   - **Certificate** — The communication between XDR and the Syslog destination can use TLS. In this case, upon connection, XDR validates that the Syslog receiver has a certificate signed by either a trusted root CA or a self signed certificate.
     - If your syslog receiver uses a `self signed CA`:
       - Browse and upload the Self Signed Syslog Receiver CA.
       - Make sure the self signed CA includes the public key.
     - If you only use a `trusted root CA`:
       - leave the Certificate field empty.
   - **Ignore Certificate Error** — XDR does not recommend, but you can choose to select this option to ignore certificate errors if they occur. This will forward alerts and logs even if the certificate contains errors.
5. **Test** the parameters to ensure a valid connection and **Create** when ready.
   1. You can define up to five Syslog servers.
   2. Upon success, the table displays the Syslog servers and their status.
6. (Optional) Manage your Syslog server connection.
   - In the Syslog Servers table
     - Locate your Syslog server and right-click to <kbd>Send text message</kbd> to test the connection.
     - XDR sends a message to the defined Syslog server, check to see if the test message indeed arrived.
   - Locate the <kbd>Status field</kbd>.
     - The Status field displays a Valid or Invalid TCP connection.
     - XDR tests connection with the Syslog server every 10min.
     - If no connection is found after 1 hour, XDR send a notice to the Notification Center.
7. Configure Notification Forwarding.
   - After you integrate with your Syslog receiver, you can configure your forwarding settings.


## Cortex Data Lake

By default, Cortex Data Lake forwards logs in CSV format and follows IETF Syslog message format defined in RFC 5425. but can select other log record formats, such as LEEF, that may adhere to different standards.
- For each instance of Cortex Data Lake, you can forward logs to ten Syslog destinations.
- The communication between Cortex Data Lake and the Syslog destination uses `Syslog over TLS`
- and upon connection Cortex Data Lake validates that the Syslog receiver has a certificate signed by a trusted root CA.
- To complete the SSL handshake and establish the connection, the Syslog receiver must present all the certificates from the chain of trust.
- Cortex Data Lake does not support self-signed certificates.


1. Enable communication between Cortex Data Lake and your Syslog receiver.
   1. Ensure the Syslog receiver can connect to Cortex Data Lake
   2. and can present a valid CA certificate to complete the connection request.
   3. Allow an inbound TLS feed to the Syslog receiver from the following IP address ranges:
   4. ![Screen Shot 2020-11-17 at 17.04.55](https://i.imgur.com/2lG48CX.png)
   5. If you have allowed specific IP addresses for inbound traffic, you must also allow the above IP address ranges to forward logs to your Syslog receiver.
   6. Obtain a certificate from a well-known, public CA, and install it on your Syslog receiver.
      1. Because Cortex Data Lake validates the server certificate to establish a connection, must verify that the Syslog receiver is configured to properly send the SSL certificate chain to Cortex Data Lake.
      2. If the app cannot verify that the certificate of the receiver and all CA's in the chain are trustworthy, the connection cannot be established.
      3. See the list of trusted certificates.
2. Sign In to the hub
3. Select the Cortex Data Lake instance to configure for Syslog forwarding.
   1. If you have multiple Cortex Data Lake instances, click the Cortex Data Lake tile and select an instance from the list of those available.
4. Select Log Forwarding > Add to add a new Syslog forwarding profile.
5. Enter a descriptive Name for the profile.
6. Enter the Syslog Server IPv4 address or FQDN.
7. Enter the Port on which the Syslog server is listening.
   - The default port for `Syslog messages over TLS` is `6514`.
8. Select the Facility.
   - Choose one of the Syslog standard values.
   - The value maps to how the Syslog server uses the facility field to manage messages.
   - For details on the facility field, see the IETF standard for the log format that you will choose in the next step.
9. Specify the Format in which to forward the logs.
   - The log format select depends on the destination of the log data.
   - For example, select LEEF if you are forwarding logs to IBM QRadar SIEM.
10. Specify the Delimiter to separate the fields in your log messages.
11. (Optional) To receive a **Status Notification** when Cortex Data Lake is unable to connect to the Syslog server, enter the email address at which you’d like to receive the notification.
    - These notifications describe the error impacting communication between Cortex Data Lake and the Syslog server, so that you can take the appropriate steps to restore Syslog connectivity.
12. (Optional) Enter a **Profile Token** to send logs to a cloud Syslog receiver.
    - If you use a third-party cloud-based Syslog service, you can enter a token that Cortex Data Lake inserts into the Syslog message so that the cloud Syslog provider can identify the source of the logs.
    - Follow your cloud Syslog provider’s instructions for generating an identifying token.
    - Enter the Profile Token.
    - Tokens have a maximum length of 128 characters.
13. Select the logs you want to forward.
    1. Add a new log filter.
    2. Select the log type.
       1. The Threat log type does not include URL logs or Data logs.
       2. If you wish to forward these log types, you must add them individually.
    3.  (Optional)Create a log filter to forward only the logs that are most critical to you.
        1. Log filters function like queries in Explore.
        2. As such, you can either write your own queries from scratch or use the query builder. Also, selecting the query field presents some common predefined queries that you can use.
        3. If you want to forward all logs of the type you selected, do not enter a query. Instead, proceed to the next step.
14. **Save** your changes.
15. Verify the **Status** of the Syslog forwarding profile is `Running`.
16. Verify that you can view logs on the Syslog receiver.
    1.  For details about the log format, refer to the Syslog field descriptions (Select the PAN-OS Administrator’s Guide for your firewall version).


---

# Filebeat

[link](https://www.jianshu.com/p/4abb141da37b)

---

# Logstash `collect data`

![Screen Shot 2020-11-16 at 19.47.26](https://i.imgur.com/BePKapW.png)


![Screen Shot 2020-11-16 at 19.48.38](https://i.imgur.com/jztRxZK.png)


输入：采集各种样式、大小和来源的数据
- 数据往往以各种各样的形式，或分散或集中地存在于很多系统中。
- Logstash 支持各种输入选择 ，可以在同一时间从众多常用来源捕捉事件。
- 能够以连续的流式传输方式，轻松地从日志、指标、Web 应用、数据存储以及各种 AWS 服务采集数据。

过滤器：实时解析和转换数据
- 数据从源传输到存储库的过程中，Logstash 过滤器能够解析各个事件，识别已命名的字段以构建结构，并将它们转换成通用格式，以便更轻松、更快速地分析和实现商业价值
- Logstash 能够动态地转换和解析数据，不受格式或复杂度的影响：
  - 利用 Grok 从非结构化数据中派生出结构
  - 从 IP 地址破译出地理坐标
  - 将 PII(个人验证信息) 数据匿名化，完全排除敏感字段
  - 整体处理不受数据源、格式或架构的影响

输出：选择你的存储，导出你的数据
- 尽管 Elasticsearch 是首选输出方向，能够为我们的搜索和分析带来无限可能，但它并非唯一选择。
- Logstash 提供众多输出选择，您可以将数据发送到您要指定的地方，并且能够灵活地解锁众多下游用例。



logstash是做数据采集的，类似于flume。  
Logstash架构：

![Screen Shot 2020-11-16 at 19.53.57](https://i.imgur.com/HDzvFSW.png)

- Batcher: 负责批量的从queue中取数据;  
- Queue分类：  
  - In Memory ： 无法处理进程Crash、机器宕机等情况，会导致数据丢失  
  - Persistent Queue In Disk：可处理进程Crash等情况，保证数据不丢失，保证数据至少消费一次，充当缓冲区，可以替代kafka等消息队列的作用。

---


## 安装


1. 下载安装

```bash
    # 下载
    [root@localhost ~]# wget https://artifacts.elastic.co/downloads/logstash/logstash-6.4.3.tar.gz
    # 解压
    [root@localhost ~]# tar -zxvf logstash-6.4.3.tar.gz -C /usr/local
    # 查看内容
    [root@localhost ~]# ls /usr/local/logstash-6.4.3/
    # 测试logstash-6.4.3
    [root@localhost logstash-6.4.3]# ./bin/logstash -e 'input{stdin{}}output{stdout{codec=>rubydebug}}'
    Sending Logstash logs to /usr/local/logstash-6.4.3/logs which is now configured via log4j2.properties
    [2019-09-25T15:12:41,903][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
    [2019-09-25T15:12:42,613][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"6.4.3"}
    [2019-09-25T15:12:45,490][INFO ][logstash.pipeline        ] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>2, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50}
    [2019-09-25T15:12:45,845][INFO ][logstash.pipeline        ] Pipeline started successfully {:pipeline_id=>"main", :thread=>"#<Thread:0x310540d4 run>"}
    The stdin plugin is now waiting for input:
    [2019-09-25T15:12:46,543][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
    [2019-09-25T15:12:47,020][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600} # 如果启动成功会出现提示语句


# 接着屏幕就等着你输入了，比如输入一个Hello World，会出现以下的提示语句。
    HelloWorld
    {
        "@timestamp" => 2019-09-25T07:14:40.491Z,
              "host" => "localhost",
          "@version" => "1",
           "message" => "HelloWorld"
    }
```


2. 配置文件简单测试  

```bash
# pipeline配置简介：  
# Pipeline用于配置input、filter和output插件

    input {
            ...
    }
    filter {
            ...
    }
    output {
            ...
    }


# 创建配置文件logstash.conf：

    [root@localhost logstash-6.4.3]# vi config/logstash.conf
    # 输入内容
    input {
        stdin { }
    }

    output {
        stdout {
            codec => rubydebug { }
        }
        elasticsearch {
            hosts => ["0.0.0.0:9200"]
            user => elastic
            password => xW9dqAxThD5U4ShQV1JT
        }
    }

    # 启动elasticsearch
    # 指定配置文件启动
    [root@localhost logstash-6.4.3]# ./bin/logstash -f config/logstash.conf
    # 同样命令行等着你输入指令
    Hello World
    {
          "@version" => "1",
              "host" => "localhost",
        "@timestamp" => 2019-09-25T07:25:03.292Z,
           "message" => "Hello World"
    }

# 访问：  
# http://192.168.77.132:9200/\_search?q=Hello
```


常见问题：

```bash
1.  错误提示：Unrecognized VM option 'UseParNewGC'  
    JDK版本不正确。

2.  (LoadError) Unsupported platform: x86\_64-linux  
    切换JDK为1.8版本

    # 安装JDK
    [root@localhost ~]# yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel
    # 切换JDK
    [root@localhost logstash-6.4.3]# alternatives --config java  

    共有 2 个提供“java”的程序。

      选项    命令
    -----------------------------------------------
     + 1           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.4.11-1.el7_7.x86_64/bin/java)
    *  2           java-1.8.0-openjdk.x86_64 (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b10-1.el7_7.x86_64/jre/bin/java)

    按 Enter 保留当前选项[+]，或者键入选项编号：2


3.  error=>"Got response code '401' contacting Elasticsearch at URL 'http://0.0.0.0:9200/'"  
    原因：  
    没有指定Elasticsearch 权限，修改配置添加elasticsearch 的用户与密码：

    output {
        stdout {
            codec => rubydebug { }
        }
        elasticsearch {
            hosts => ["0.0.0.0:9200"]
            user => elastic
            password => xW9dqAxThD5U4ShQV1JT
        }
    }


2.  YUM安装  
    参考档安装：  
    [https://www.elastic.co/guide/en/logstash/6.4/installing-logstash.html#\_yum](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.elastic.co%2Fguide%2Fen%2Flogstash%2F6.4%2Finstalling-logstash.html%23_yum)
```


### 采集MySql数据库数据

```bash
1.  下载MySql-JDBC  
    MySql-JDBC下载地址：[https://dev.mysql.com/downloads/connector/j/5.1.html](https://links.jianshu.com/go?to=https%3A%2F%2Fdev.mysql.com%2Fdownloads%2Fconnector%2Fj%2F5.1.html)  

    MySql-JDBC下载地址


    5.1下载：

    [root@localhost ~]# wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.48.tar.gz
     解压：
    [root@localhost ~]# tar -zxvf mysql-connector-java-5.1.48.tar.gz



8.0下载地址：  
[https://dev.mysql.com/downloads/connector/j/8.0.html](https://links.jianshu.com/go?to=https%3A%2F%2Fdev.mysql.com%2Fdownloads%2Fconnector%2Fj%2F8.0.html)  

8.0下载地址

    [root@localhost ~]# wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-8.0.17.tar.gz



2.  安装MySql-JDBC插件

    #   安装插件
    [root@localhost logstash-6.4.3]# ./bin/logstash-plugin install logstash-input-jdbc
    Validating logstash-input-jdbc
    Installing logstash-input-jdbc
    Installation successful



3.  创建数据库、表、与sql文件  
    数据库：log\_test 表：news



    数据库与表



    数据脚本：


    SET NAMES utf8mb4;
    SET FOREIGN_KEY_CHECKS = 0;

    -- ----------------------------
    -- Table structure for news
    -- ----------------------------
    DROP TABLE IF EXISTS `news`;
    CREATE TABLE `news`  (
      `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键',
      `title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL COMMENT '标题',
      `content` text CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL COMMENT '内容',
      PRIMARY KEY (`id`) USING BTREE
    ) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci ROW_FORMAT = Dynamic;

    SET FOREIGN_KEY_CHECKS = 1;



创建执行SQL:

    # 创建目录
    [root@localhost logstash-6.4.3]# mkdir sql
    # 添加内容
    [root@localhost sql]# vi sql/jdbc.sql
    # 内容为
    select * from news


要注意：这里的内容就是logstash依赖执行的sql命令，所以这里的表名要跟你实际的数据库表名一致，否则会失败。  
至于jdbc.sql的内容就是你的业务sql，只能有一条，且末尾不要加分号，否则出错！

4.  单数据源同步  
    单数据源同步是指，数据只写入一个index下（注意：6.x版本下，一个index下只能有一个type），jdbc块和elasticsearch块也是 一 一 对应的关系，具体看下同步conf的配置（示例配置文件名称： singledb.conf）：

    # 创建配置文件
    [root@localhost logstash-6.4.3]# vi config/singledb.conf
    # 内容
    input {
        stdin {
        }
        jdbc {
          # mysql jdbc connection 连接地址
          jdbc_connection_string => "jdbc:mysql://10.2.129.166:3306/log_test?serverTimezone=Asia/Shanghai&useSSL=true&useUnicode=true&characterEncoding=UTF-8"
          # 登录数据库的用户名、密码
          jdbc_user => "root"
          jdbc_password => "1234"
          # jdbc 驱动包路径
          jdbc_driver_library => "/usr/local/logstash-6.4.3/config/mysql-connector-java-8.0.17.jar"      # 连接驱动类名
          jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
          jdbc_paging_enabled => "true"
          jdbc_page_size => "50000"
          statement_filepath => "/usr/local/logstash-6.4.3/config/jdbc.sql"
          # 以下表示定时执行任务，使用cron表达式，本文只全量同步一次，所以不配置定时器，如果要实现增量更新，需要配合定时器以及上次查询的最后一个值，具体要根据你的业务来定。
          # schedule => "* * * * *"
          type => "jdbc"
        }
    }
    filter {
        json {
            source => "message"
            remove_field => ["message"]
        }
    }
    output {
        elasticsearch {
            hosts => ["192.168.77.132:9200"]
            index => "mynews"
            document_id => "%{id}"
            user => elastic
            password => xW9dqAxThD5U4ShQV1JT
        }
        stdout {
            codec => json_lines
        }
    }

    #  复制jdbc jar  文件
    [root@localhost logstash-6.4.3]# cp /root/mysql-connector-java-8.0.17/mysql-connector-java-8.0.17.jar config

    # 启动同步
    [root@localhost logstash-6.4.3]# ./bin/logstash -f config/singledb.conf



`注意：配置文件中的jar与sql一定要使用绝对路径。`  
启动同步，同步的内容会输出到屏幕上。执行完成后可在es中查看数据。

同步结果



kibana中查看



结果

2.  多数据源同步  
    多数据源同步是指，需要同步多种类型的数据到es中，input的配置添加相应的jdbc模块，output中根据type类型判断添加对应的elasticsearch模块：

    input {
        jdbc {
            jdbc_connection_string => "jdbc:mysql://192.168.91.149:3306/test"
            jdbc_user => "root"
            jdbc_password => "1234"
            # 此处的路径最好是绝对路径，相对路径取决与允许命令的目录
            jdbc_driver_library => "/usr/local/logstash-6.4.3/config/mysql-connector-java-8.0.17.jar"
            jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
            jdbc_paging_enabled => "true"
            jdbc_page_size => "50000"
            # 此处的路径最好是绝对路径，相对路径取决与允许命令的目录
            statement_filepath =>  "/usr/local/logstash-6.4.3/config/jdbc0.sql"
            type => "user"
        }
        jdbc {
            jdbc_connection_string => "jdbc:mysql://192.168.91.149:3306/test"
            jdbc_user => "root"
            jdbc_password => "1234"
            # 此处的路径最好是绝对路径，相对路径取决与允许命令的目录
            jdbc_driver_library => "/usr/local/logstash-6.4.3/config/mysql-connector-java-8.0.17.jar"
            jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
            jdbc_paging_enabled => "true"
            jdbc_page_size => "50000"
            # 此处的路径最好是绝对路径，相对路径取决与允许命令的目录
            statement_filepath =>  "/usr/local/logstash-6.4.3/config/jdbc1.sql"
            type => "user_address"
        }
    }
    output {
        #设置窗口日志输出
        stdout {
            codec => json_lines
        }
           # 与JDBC定义的type对应
        if[type] == "user" {
            elasticsearch {
                hosts => ["192.168.77.132:9200"]
                # 注意index的值不支持大写字母
                index => "user"
                # document_type自行设置，不设置时，默认为doc
                # document_type => ""
                # 此处的值来自查询sql中的列名称，根据需要自行配置
                document_id => "%{id}"
                           user => elastic
                           password => xW9dqAxThD5U4ShQV1JT
            }
        }
        if[type] == "user_news" {
            elasticsearch {
                hosts => ["192.168.77.132:9200"]
                # 注意index的值不支持大写字母
                index => "user_news"
                # document_type自行设置，不设置时，默认为doc
                # document_type => ""
                # 此处的值来自查询sql中的列名称，根据需要自行配置
                document_id => "%{id}"
                           user => elastic
                           password => xW9dqAxThD5U4ShQV1JT
            }
        }

    }



3.  全量同步  
    以上的单数据源/多数据源同步都是全量同步，即没有任何条件地进行同步。

4.  增量同步  
    增量同步需要在jdbc模块添加相应的增量配置


> 配置参数  
> Schedule:是cron格式的同步周期，其它几个都是用来记录同步增量指标的，  
> Tracking\_column:是数据库中的增量指标字段名  
> Tracking\_columu\_type:目前只支持两种numeric,timestamp，  
> Last\_run\_metadata\_path:是保存上次同步的增量指标值。  
> 而 :sql\_last\_value如果input里面use\_column\_value => true， 即如果设置为true的话，可以是我们设定的字段的上一次的值。  
> 默认 use\_column\_value => false， 这样 :sql\_last\_value为上一次更新的最后时刻值。  
> 也就是说，对于新增的值，才会更新。这样就实现了增量更新的目的。
```


### 相关配置

```bash
1.  持久队列基本配置(pipelines.yml)

    queue.type:persisted    # 默认是memory
    queue.max_bytes:4gb     # 队列存储最大数据量


2.  线程相关配置(logstash.yml)

    pipeline.worksers | -w
    # pipeline线程数，即filter_output的处理线程数，默认是cpu核数
    pipeline.batch.size | -b
    # Batcher一次批量获取的待处理文档数，默认是125，可以根据输出进行调整，越大会占用越多的heap空间，可以通过jvm.options调整
    pipeline.batch.delay | -u
    # Batcher等待的时长，单位为ms



3.  Logstash配置文件:  
    logstash设置相关的配置文件（在conf文件夹中）  
    logstash.yml：logstash相关配置，比如node.name、path.data、pipeline.workers、queue.type等，这其中的配置可以被命令行参数中的相关参数覆盖  
    jvm.options：修改jvm的相关参数，比如修改heap size等  
    pipeline配置文件：定义数据处理流程的文件，以.conf结尾  
    logstash.yml配置项：

    node.name:   节点名称，便于识别
    path.data:   持久化存储数据的文件夹，默认是logstash home目录下的data
    path.config: 设定pipeline配置文件的目录（如果指定文件夹，会默认把文件夹下的所有.conf文件按照字母顺序拼接为一个文件）
    path.log:    设定pipeline日志文件的目录
    pipeline.workers: 设定pipeline的线程数（filter+output），优化的常用项
    pipeline.batch.size/delay: 设定批量处理数据的数据和延迟
    queue.type: 设定队列类型，默认是memory
    queue.max_bytes: 队列总容量，默认是1g
```


### 常见问题

```bash
1.  Error: Java::JavaSql::SQLException: null, message from server: "Host 'DESKTOP-T92R1EE' is not allowed to connect to this MySQL server"  
    数据库没有开启远程连接

    select user,host from user;
    # 在Mysql数据库中执行以下sql
    update user set host='%' where user ='root';
    flush privileges;


最好重启下Mysql



执行SQL

### 推荐阅读[更多精彩内容](/)

*   [Elastic+logstash+head简单介绍](/p/b0abc1591b69)

    Elastic+logstash+head简单介绍 一. 概述 ElasticSearch是一个基于Lucene的...

    [柒月失凄](/u/a62227da9a51)阅读 2,747评论 0赞 4

*   [ES 译文之如何使用 Logstash 实现关系型数据库与 ElasticSearch 之间的...](/p/f576721a5c08)

    译者前言 近期的主要工作是在为公司的 APP 增加搜索功能。因为也遇到了需要把关系型数据库中的数据同步 Elast...

    [java高并发](/u/e2c8c5808732)阅读 1,161评论 0赞 2

*   [mysql数据同步elasticsearch（es）全文检索容器；其它数据库同理](/p/9d8224c8272f)

    一、安装ElasticSearch(下面统称es,版本6.0.0,环境windows10) 直接上下载地址：htt...

    [AaChoxsu](/u/0c95e0282cba)阅读 1,415评论 0赞 7

*   [ELK技术栈-Logstash实战案例](/p/0d829c410651)

    本文作者：罗海鹏，叩丁狼高级讲师。原创文章，转载请注明出处。 案例一：收集Nginx访问日志数据 Nginx是企业...

    [叩丁狼教育](/u/231b43e2c05f)阅读 1,365评论 0赞 2

    [](/p/0d829c410651)
*   [（十一）ELK技术栈之-Logstash实战案例](/p/448464c50fe1)

    案例一：收集Nginx访问日志数据 Nginx是企业中常用的http服务器，也有人称它为反向代理服务器和负债均衡服...

    [LuoHaiPeng](/u/95006ccf2673)阅读 339评论 0赞 3

    [](/p/448464c50fe1)
```




ref:
- [blog](https://www.jianshu.com/p/140ce103b03d)













.
